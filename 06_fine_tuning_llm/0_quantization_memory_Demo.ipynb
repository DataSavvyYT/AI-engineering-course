{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMLTkkite+hGDuxzhqGbG0B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DataSavvyYT/AI-engineering-course/blob/main/06_fine_tuning_llm/0_quantization_memory_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install necessary libraries (quiet mode)\n",
        "!pip install -q transformers torch accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "430zhOo5Powm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import os"
      ],
      "metadata": {
        "id": "tHSCVd-NqFQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_size_mb(model):\n",
        "    \"\"\"\n",
        "    Calculate the memory footprint of a model in Megabytes (MB).\n",
        "    Handles both Hugging Face models and raw PyTorch quantized modules.\n",
        "    \"\"\"\n",
        "    # If it's a Hugging Face model, use their built-in efficient method\n",
        "    if hasattr(model, \"get_memory_footprint\"):\n",
        "        return model.get_memory_footprint() / 1024 / 1024\n",
        "\n",
        "    # Fallback for raw PyTorch quantized models (manual calculation)\n",
        "    mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])\n",
        "    mem_buffers = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n",
        "    return (mem_params + mem_buffers) / 1024 / 1024"
      ],
      "metadata": {
        "id": "hu2bPh5qW-Fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flush():\n",
        "    \"\"\"Cleans up memory to ensure a fair comparison.\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "mVTs-GQDXDTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a small model for the demo (OPT-125M is small and fast to download)\n",
        "model_id = \"facebook/opt-125m\"\n",
        "print(f\"--- Comparision for Model: {model_id} ---\\n\")"
      ],
      "metadata": {
        "id": "gFoKP_JNXF4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# PHASE 1: Load Non-Quantized Model (FP32)\n",
        "# ==========================================\n",
        "print(\"1. Loading Standard Model (Float32)...\")\n",
        "try:\n",
        "    model_std = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float32)\n",
        "    size_std = get_model_size_mb(model_std)\n",
        "    print(f\"   Standard Model Size: {size_std:.2f} MB\")\n",
        "\n",
        "    # Cleanup to free memory for next step\n",
        "    del model_std\n",
        "    flush()\n",
        "except Exception as e:\n",
        "    print(f\"Error loading standard model: {e}\")\n",
        "\n",
        "print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "P0zLlqieXIkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# PHASE 2: Load Quantized Model\n",
        "# ==========================================\n",
        "if torch.cuda.is_available():\n",
        "    # GPU PATH: Use bitsandbytes 4-bit Quantization (Modern LLM approach)\n",
        "    print(\"2. GPU Detected. Loading with 4-bit Quantization (bitsandbytes)...\")\n",
        "\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        model_quant = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        size_quant = get_model_size_mb(model_quant)\n",
        "        print(f\"   Quantized Model Size (4-bit): {size_quant:.2f} MB\")\n",
        "\n",
        "        # Calculate savings\n",
        "        savings = size_std / size_quant\n",
        "        print(f\"\\nResult: The quantized model is ~{savings:.1f}x smaller than the standard model.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during GPU quantization: {e}\")\n",
        "\n",
        "else:\n",
        "    # CPU PATH: Use PyTorch Dynamic Quantization (Int8)\n",
        "    print(\"2. No GPU Detected. Using PyTorch Dynamic Quantization (Int8)...\")\n",
        "\n",
        "    try:\n",
        "        # Reload standard model to apply dynamic quantization\n",
        "        model_to_quantize = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "\n",
        "        # Apply dynamic quantization to Linear layers\n",
        "        model_quant = torch.quantization.quantize_dynamic(\n",
        "            model_to_quantize,\n",
        "            {torch.nn.Linear},\n",
        "            dtype=torch.qint8\n",
        "        )\n",
        "\n",
        "        size_quant = get_model_size_mb(model_quant)\n",
        "        print(f\"   Quantized Model Size (Int8): {size_quant:.2f} MB\")\n",
        "\n",
        "        # Calculate savings\n",
        "        savings = size_std / size_quant\n",
        "        print(f\"\\nResult: The quantized model is ~{savings:.1f}x smaller than the standard model.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during CPU quantization: {e}\")"
      ],
      "metadata": {
        "id": "meLryNSGXMKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Eq4q24QAXUfM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}